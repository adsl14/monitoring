{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train_tfRecord-example.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "CC5_T4yvem96",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import auth, drive\n",
        "\n",
        "auth.authenticate_user()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "evcUG83CfJLn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!earthengine authenticate"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i3nS1YsKe2ar",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import the Earth Engine API and initialize it.\n",
        "import ee\n",
        "import time\n",
        "import os\n",
        "from pprint import pprint\n",
        "import numpy\n",
        "import tensorflow as tf\n",
        "tf.enable_eager_execution()\n",
        "\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "\n",
        "# Montamos la unidad de trabajo de nuestro drive\n",
        "drive.mount('/content/drive/')\n",
        "\n",
        "# Cambiamos la ruta de trabajo a nuestro drive\n",
        "os.chdir('/content/drive/My Drive/')\n",
        "\n",
        "ee.Initialize()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SkdugVz44wg2",
        "colab_type": "text"
      },
      "source": [
        "Funciones a utilizar"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fNpUN9IL4yVh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Cloud masking function.\n",
        "def maskL8sr(image):\n",
        "  cloudShadowBitMask = ee.Number(2).pow(3).int()\n",
        "  cloudsBitMask = ee.Number(2).pow(5).int()\n",
        "  qa = image.select('pixel_qa')\n",
        "  mask = qa.bitwiseAnd(cloudShadowBitMask).eq(0).And(\n",
        "    qa.bitwiseAnd(cloudsBitMask).eq(0))\n",
        "  return image.updateMask(mask).select(bands).divide(10000)\n",
        "\n",
        "def parse_tfrecord(example_proto):\n",
        "  \"\"\"The parsing function.\n",
        "\n",
        "  Read a serialized example into the structure defined by featuresDict.\n",
        "\n",
        "  Args:\n",
        "    example_proto: a serialized Example.\n",
        "  \n",
        "  Returns: \n",
        "    A tuple of the predictors dictionary and the label, cast to an `int32`.\n",
        "  \"\"\"\n",
        "  parsed_features = tf.io.parse_single_example(example_proto, featuresDict)\n",
        "  labels = parsed_features.pop(label)\n",
        "  return parsed_features, tf.cast(labels, tf.int32)\n",
        "\n",
        "def normalizedDifference(a, b):\n",
        "  \"\"\"Compute normalized difference of two inputs.\n",
        "\n",
        "  Compute (a - b) / (a + b).  If the denomenator is zero, add a small delta.  \n",
        "\n",
        "  Args:\n",
        "    a: an input tensor with shape=[1]\n",
        "    b: an input tensor with shape=[1]\n",
        "\n",
        "  Returns:\n",
        "    The normalized difference as a tensor.\n",
        "  \"\"\"\n",
        "  nd = (a - b) / (a + b)\n",
        "  nd_inf = (a - b) / (a + b + 0.000001)\n",
        "  return tf.where(tf.math.is_finite(nd), nd, nd_inf)\n",
        "\n",
        "def addNDVI(features, label):\n",
        "  \"\"\"Add NDVI to the dataset.\n",
        "  Args: \n",
        "    features: a dictionary of input tensors keyed by feature name.\n",
        "    label: the target label\n",
        "  \n",
        "  Returns:\n",
        "    A tuple of the input dictionary with an NDVI tensor added and the label.\n",
        "  \"\"\"\n",
        "  features['NDVI'] = normalizedDifference(features['B5'], features['B4'])\n",
        "  return features, label\n",
        "\n",
        "# Keras requires inputs as a tuple.  Note that the inputs must be in the\n",
        "# right shape.  Also note that to use the categorical_crossentropy loss,\n",
        "# the label needs to be turned into a one-hot vector.\n",
        "def toTuple(dict, label):\n",
        "  return tf.transpose(list(dict.values())), tf.one_hot(indices=label, depth=nClasses)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q4XRmEA0nXYY",
        "colab_type": "text"
      },
      "source": [
        "Ejemplo 1.1 - Preprocesar datos de Earth Engine y transformarlos a tfRecord."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQ-Fo0j8nWBL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use these bands for prediction.\n",
        "bands = ['B2', 'B3', 'B4', 'B5', 'B6', 'B7']\n",
        "label = 'landcover'\n",
        "\n",
        "# Use Landsat 8 surface reflectance data.\n",
        "l8sr = ee.ImageCollection('LANDSAT/LC08/C01/T1_SR')\n",
        "# The image input data is a 2018 cloud-masked median composite.\n",
        "image = l8sr.filterDate('2018-01-01', '2018-12-31').map(maskL8sr).median()\n",
        "\n",
        "# Change the following two lines to use your own training data.\n",
        "labels = ee.FeatureCollection('GOOGLE/EE/DEMOS/demo_landcover_labels')\n",
        "\n",
        "# Sample the image at the points and add a random column.\n",
        "sample = image.sampleRegions(collection=labels, properties=[label], scale=30).randomColumn()\n",
        "\n",
        "# Partition the sample approximately 70-30.\n",
        "training = sample.filter(ee.Filter.lt('random', 0.7))\n",
        "testing = sample.filter(ee.Filter.gte('random', 0.7))\n",
        "\n",
        "# Names for output files.\n",
        "trainFilePrefix = 'Training_demo'\n",
        "testFilePrefix = 'Testing_demo'\n",
        "fileNameSuffix = '.tfrecord.gz'\n",
        "\n",
        "# This is list of all the properties we want to export.\n",
        "featureNames = list(bands)\n",
        "featureNames.append(label)\n",
        "\n",
        "trainingTask = ee.batch.Export.table.toDrive(\n",
        "    collection=training,\n",
        "    folder='agricultura/bands tfrecord',\n",
        "    description='Training Export', \n",
        "    fileNamePrefix=trainFilePrefix, \n",
        "    fileFormat='TFRecord',\n",
        "    selectors=featureNames)\n",
        "\n",
        "testingTask = ee.batch.Export.table.toDrive(\n",
        "    collection=testing,\n",
        "    folder='agricultura/bands tfrecord',\n",
        "    description='Testing Export', \n",
        "    fileNamePrefix=testFilePrefix, \n",
        "    fileFormat='TFRecord',\n",
        "    selectors=featureNames)\n",
        "\n",
        "# Start the tasks.\n",
        "trainingTask.start()\n",
        "testingTask.start()\n",
        "\n",
        "# Poll the training task until it's done.\n",
        "while trainingTask.active():\n",
        "  print('Polling for task (id: {}).'.format(trainingTask.id))\n",
        "  time.sleep(30)\n",
        "print('Done with training export.')\n",
        "\n",
        "# Poll the testing task until it's done.\n",
        "while testingTask.active():\n",
        "  print('Polling for task (id: {}).'.format(testingTask.id))\n",
        "  time.sleep(30)\n",
        "print('Done with testing export.')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ej8cOUUrt7D9",
        "colab_type": "text"
      },
      "source": [
        "Ejemplo 1.2 - Obtener el fichero tfRecord y prepararlo para tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Gp-OO3WuF5k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get the tfRecord from Google drive\n",
        "trainFilePath = 'agricultura/bands tfrecord' + trainFilePrefix + fileNameSuffix\n",
        "testFilePath = 'agricultura/bands tfrecord' + testFilePrefix + fileNameSuffix\n",
        "\n",
        "# Create a dataset from the TFRecord file in Cloud Storage.\n",
        "trainDataset = tf.data.TFRecordDataset(trainFilePath, compression_type='GZIP')\n",
        "testDataset = tf.data.TFRecordDataset(testFilePath, compression_type='GZIP')\n",
        "\n",
        "# Print the records to check.\n",
        "# print(iter(trainDataset).next())\n",
        "# print(iter(testDataset).next())\n",
        "\n",
        "# List of fixed-length features, all of which are float32.\n",
        "columns = [tf.io.FixedLenFeature(shape=[1], dtype=tf.float32) for k in featureNames]\n",
        "\n",
        "# Dictionary with names as keys, features as values.\n",
        "featuresDict = dict(zip(featureNames, columns))\n",
        "\n",
        "# Map the function over the dataset.\n",
        "parsedDatasetTrain = trainDataset.map(parse_tfrecord, num_parallel_calls=5)\n",
        "parsedDatasetTest = testDataset.map(parse_tfrecord, num_parallel_calls=5)\n",
        "\n",
        "# Print the first parsed record to check.\n",
        "# pprint(iter(parsedDatasetTrain).next())\n",
        "# pprint(iter(parsedDatasetTest).next())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ljyI-jCOogvU",
        "colab_type": "text"
      },
      "source": [
        "Ejemplo 1.3 - Crear el modelo y empezar a entrenar"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0XJ_NCx-ojYs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "outputId": "4a79ccc6-8909-45c0-920f-7bee9d0f7da8"
      },
      "source": [
        "# How many classes there are in the model.\n",
        "nClasses = 3\n",
        "labelsName= ['bare', 'vegetation', 'water']\n",
        "batches = 10\n",
        "nEpochs = 3\n",
        "stepPerEpochs = 100\n",
        "\n",
        "# Add NDVI.\n",
        "inputTrainDataset = parsedDatasetTrain.map(addNDVI)\n",
        "inputTestDataset = parsedDatasetTest.map(addNDVI)\n",
        "\n",
        "# Repeat the input dataset as many times as necessary in batches of 10.\n",
        "inputTrainDataset = inputTrainDataset.map(toTuple).repeat().batch(batches)\n",
        "inputTestDataset = inputTestDataset.map(toTuple).batch(1)\n",
        "\n",
        "# Define the layers in the model.\n",
        "model = Sequential()\n",
        "model.add(Dense(64, activation=tf.nn.relu))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(nClasses, activation=tf.nn.softmax))\n",
        "\n",
        "# Compile the model with the specified loss function.\n",
        "model.compile(optimizer=tf.train.AdamOptimizer(),loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "\n",
        "# Fit the model to the training data.\n",
        "# Don't forget to specify `steps_per_epoch` when calling `fit` on a dataset.\n",
        "model.fit(x=inputTrainDataset.shuffle(50), epochs=nEpochs, steps_per_epoch=stepPerEpochs)\n",
        "print(\"Training finished\")\n",
        "\n",
        "# Evaluate the model\n",
        "model.evaluate(x=inputTestDataset, steps=100)\n",
        "print(\"Evaluation finished\")"
      ],
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "100/100 [==============================] - 2s 17ms/step - loss: 0.9138 - acc: 0.5603\n",
            "Epoch 2/3\n",
            "100/100 [==============================] - 1s 7ms/step - loss: 0.6236 - acc: 0.9038\n",
            "Epoch 3/3\n",
            "100/100 [==============================] - 1s 7ms/step - loss: 0.4162 - acc: 0.9533\n",
            "Training finished\n",
            " 23/100 [=====>........................] - ETA: 0s - loss: 0.4578 - acc: 0.9130 WARNING:tensorflow:Your dataset ran out of data; interrupting training. Make sure that your dataset can generate at least `steps * epochs` batches (in this case, 100 batches). You may need to use the repeat() function when building your dataset.\n",
            "Evaluation finished\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ARSf_wme7Ma",
        "colab_type": "code",
        "outputId": "5c07fe9a-8014-47fa-9a3a-7a16523faed6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "def NDVI(entrada):\n",
        "    ndvi = entrada.addBands(entrada.normalizedDifference(['B8','B4']).rename('NDVI'))\n",
        "    return (ndvi)\n",
        "\n",
        "bands = ['B4','B8']\n",
        "label = 'agriculture _activity'\n",
        "\n",
        "parcelas = ee.FeatureCollection(\"ft:1DLwfbRr4i4fDh0i-Zy8mB59F5L_HOlqojE0JHDYn\")\n",
        "coleccion_ndvi = ee.ImageCollection('COPERNICUS/S2').filterBounds(parcelas).filterDate('2017‐09‐01','2018‐08‐31').filterMetadata('CLOUD_COVERAGE_ASSESSMENT','less_than',5).map(NDVI).select('NDVI')\n",
        "\n",
        "image = coleccion_ndvi.median()\n",
        "\n",
        "# Sample the image at the points and add a random column.\n",
        "sample = image.sampleRegions(collection=parcelas, properties=[label], scale=30).randomColumn()\n",
        "\n",
        "# Partition the sample approximately 70-30.\n",
        "training = parcelas.filter(ee.Filter.lt('random', 0.7))\n",
        "testing = parcelas.filter(ee.Filter.gte('random', 0.7))\n",
        "\n",
        "\n",
        "from pprint import pprint\n",
        "\n",
        "# Print the first couple points to verify.\n",
        "pprint({'training': training.first().getInfo()})\n",
        "pprint({'testing': testing.first().getInfo()})\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'training': None}\n",
            "{'testing': None}\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}